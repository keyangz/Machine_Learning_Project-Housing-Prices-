{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def transform_features(df):\n",
    "\n",
    "    ##### MSSubClass #####\n",
    "    # Convert to string, since it's a code\n",
    "    df['MSSubClass'] = df['MSSubClass'].astype(str)\n",
    "\n",
    "    ##### OverallCond #####\n",
    "    # Convert to string, since it doesn't look linearly related to price\n",
    "    df['OverallCond'] = df['OverallCond'].astype(str)\n",
    "\n",
    "    ##### LotFrontage #####\n",
    "    # Impute missing value with mean\n",
    "    df['LotFrontage'] = df['LotFrontage'].fillna(df['LotFrontage'].mean())\n",
    "    # Add squared term to account for non-linearity\n",
    "    df['sqLotFrontage'] = df['LotFrontage'] ** 2\n",
    "\n",
    "    ##### LotArea #####\n",
    "    # Add square-root term based on shape of date wrt response\n",
    "    df['sqrtLotArea'] = np.sqrt(df['LotArea'])\n",
    "\n",
    "    ##### YearBuilt #####\n",
    "    # Add square term\n",
    "    df['sqYearBuilt'] = df['YearBuilt'] ** 2\n",
    "\n",
    "    ##### YearBuilt #####\n",
    "    # Add square term\n",
    "    df['sqYearBuilt'] = df['YearBuilt'] ** 2\n",
    "\n",
    "    ##### MasVnrArea #####\n",
    "    # Impute missing values for MasVnrArea\n",
    "    df['MasVnrArea'] = df['MasVnrArea'].fillna(df['MasVnrArea'].mean())\n",
    "\n",
    "    ##### BsmtFinSF1 #####\n",
    "    # Add squared term to account for no basement case non-linearity\n",
    "    df['sqBsmtFinSF1'] = df['BsmtFinSF1'] ** 2\n",
    "\n",
    "    ##### BsmtUnfSF #####\n",
    "    # Add squared term to account for no basement case non-linearity\n",
    "    df['sqBsmtUnfSF'] = df['BsmtUnfSF'] ** 2\n",
    "\n",
    "    ##### GrLivArea #####\n",
    "    # Add squared term for non-linearity.\n",
    "    df['GrLivArea_sq'] = df['GrLivArea'] ** 2\n",
    "\n",
    "    ##### FullBath #####\n",
    "    # Impute 0 values with mean\n",
    "    df['FullBath'] = np.where((df['FullBath'] == 0), df['FullBath'].mean(), df['FullBath'])\n",
    "\n",
    "    ##### BedroomAbvGr #####\n",
    "    # Impute 0 values with mean\n",
    "    df['BedroomAbvGr'] = np.where((df['BedroomAbvGr']==0), df['BedroomAbvGr'].mean(), df['BedroomAbvGr'])\n",
    "\n",
    "    ##### add square term for garage year built\n",
    "    df['sqGarageYrBlt'] = df['GarageYrBlt'] ** 2\n",
    "\n",
    "    ##### add square term of GarageCars #####\n",
    "    df['sqGarageCars'] = df['GarageCars'] ** 2\n",
    "\n",
    "    ##### add square term of Garage Area #####\n",
    "    df['sqGarageArea'] = df['GarageArea'] ** 2\n",
    "\n",
    "    ##### add square term for WoodDeckSF #####\n",
    "    df['sqWoodDeckSF'] = df['WoodDeckSF'] ** 2\n",
    "\n",
    "    ##### add square term for OpenPorchSF #####\n",
    "    df['sqOpenPorchSF'] = df['OpenPorchSF'] ** 2\n",
    "\n",
    "    ##### add square term for EnclosedPorch #####\n",
    "    df['sqEnclosedPorch'] = df['EnclosedPorch'] ** 2\n",
    "\n",
    "    ##### add square term for ScreenPorch #####\n",
    "    df['sqScreenPorch'] = df['ScreenPorch'] ** 2\n",
    "\n",
    "    ##### add square term for 3SsnPorch #####\n",
    "    df['sq3SsnPorch'] = df['3SsnPorch'] ** 2\n",
    "\n",
    "    ##### add square term for PoolArea #####\n",
    "    df['sqPoolArea'] = df['PoolArea'] ** 2\n",
    "\n",
    "    ##### add square term of MoSold #####\n",
    "    df['sqMoSold'] = df['MoSold'] ** 2\n",
    "\n",
    "    ##### add square term of YrSold #####\n",
    "    df['sqYrSold'] = df['YrSold'] ** 2\n",
    "\n",
    "    ##### PoolQC #####\n",
    "    df['HasPool'] = pd.notnull(df['PoolQC']).astype('int')\n",
    "    # Drop this column, there's not enough data in any category\n",
    "    df= df.drop(['PoolQC'], 1)\n",
    "\n",
    "    ##### Id #####\n",
    "    # Drop this column, it's just an identifier\n",
    "    df = df.drop(['Id'], 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def transform_target(df):\n",
    "    ##### Response = SalePrice #####\n",
    "    df['logSalePrice'] = np.log(df['SalePrice'])\n",
    "    df = df.drop('SalePrice', 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Remove huge basement outlier from training data\n",
    "df_train = df_train.drop(df_train[df_train['TotalBsmtSF'] > 4000].index)\n",
    "\n",
    "# Split train into x and y\n",
    "df_train_y = df_train.loc[:, ['SalePrice']]\n",
    "df_train_x = df_train.drop(['SalePrice'], 1)\n",
    "\n",
    "\n",
    "# Feature Engineering\n",
    "df_train_y = transform_target(df_train_y) # train y\n",
    "df_train_x = transform_features(df_train_x) # train x\n",
    "df_test_x = transform_features(df_test) # test x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For categorical features, remove any categories with less than 3 values\n",
    "categorical_cols = list(df_train_x.select_dtypes(include=['object']).columns.values)\n",
    "for col in categorical_cols:\n",
    "    category_to_remove = list(df_train_x[col].value_counts()[df_train_x[col].value_counts() <= 2].index)\n",
    "    if category_to_remove:\n",
    "        df_train_x[col] = df_train_x[col].replace(category_to_remove, np.nan)\n",
    "        df_test_x[col] = df_test_x[col].replace(category_to_remove, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For numeric feature, log transform skewed distributions\n",
    "from scipy.stats import skew\n",
    "numeric_cols = list(df_train_x.select_dtypes(exclude=['object']).columns.values)\n",
    "for col in numeric_cols:\n",
    "    if skew(df_train_x[col]) > 0.75 or skew(df_train_x[col]) < -0.75:\n",
    "        df_train_x[col] = np.log1p(df_train_x[col])\n",
    "        df_test_x[col] = np.log1p(df_test_x[col])\n",
    "\n",
    "# Encode Categorical Variables and put into sci-kit friendly format\n",
    "df_train_x = pd.get_dummies(df_train_x, drop_first=True, dummy_na=False)\n",
    "df_train_x = df_train_x.fillna(0)\n",
    "df_test_x = pd.get_dummies(df_test_x, drop_first=True, dummy_na=False)\n",
    "df_test_x = df_test_x.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensure test and training have the same variables\n",
    "test_cols = set(df_test_x.columns.values)\n",
    "train_cols = set(df_train_x.columns.values)\n",
    "# Remove cols from train that are not in test\n",
    "df_train_x = df_train_x.drop(list(train_cols-test_cols),1)\n",
    "# Remove cols from train that are not in test\n",
    "df_test_x = df_test_x.drop(list(test_cols-train_cols),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Double check that all columns are the same in test and train\n",
    "for test_col, train_col in zip(list(df_test_x.columns.values),list(df_train_x.columns.values)):\n",
    "    if test_col != train_col:\n",
    "        print test_col, train_col, 'not the same'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert to matrix\n",
    "train_x = df_train_x.as_matrix()\n",
    "train_y = df_train_y.as_matrix().ravel()\n",
    "test_x = df_test_x.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoosting Regression\n",
      "The top ten most significant coefficients for random xg boost regression are:\n",
      "LotFrontage 28\n",
      "LotArea 19\n",
      "OverallQual 19\n",
      "YearBuilt 16\n",
      "YearRemodAdd 15\n",
      "MasVnrArea 15\n",
      "BsmtFinSF1 14\n",
      "BsmtFinSF2 12\n",
      "BsmtUnfSF 12\n",
      "TotalBsmtSF 10\n",
      "Pipeline(steps=[('xg', XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=50, nthread=-1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1))])\n",
      "\n",
      "Method is XGBoost Regression\n",
      "Root Mean Square Error 0.157798122244\n",
      "\n",
      "#################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########### XGBoosting Regression #############\n",
    "\n",
    "print \"XGBoosting Regression\"\n",
    "from xgboost import XGBRegressor\n",
    "from utils import optimize_and_evaluate_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "import operator\n",
    "\n",
    "pipeline_xg = Pipeline(steps=[('xg',XGBRegressor(n_estimators=50,learning_rate=1))])\n",
    "parameters_xg = {'xg__n_estimators': (40, 50), 'xg__learning_rate': (0.1, 1)}\n",
    "\n",
    "# fit the model, find the index of the top 10 features and their scores.\n",
    "pipeline_xg.fit(train_x,train_y)\n",
    "score = pipeline_xg.named_steps['xg'].booster().get_fscore()\n",
    "sorted_score_top10 = sorted(score.items(), key=operator.itemgetter(1), reverse=True)[0:10]\n",
    "index = [int(pair[0][1:]) for pair in sorted_score_top10]\n",
    "feature_score = [int(pair[1]) for pair in sorted_score_top10]\n",
    "\n",
    "# Print features by importance in model\n",
    "print\"The top ten most significant coefficients for random xg boost regression are:\"\n",
    "for i in range(len(index)):\n",
    "    print df_train_x.columns[i], feature_score[i]\n",
    "\n",
    "# Search for best parameters\n",
    "optimize_and_evaluate_model(pipeline_xg, parameters_xg, \"XGBoost Regression\", train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top ten most significant coefficients for lasso regression are:\n",
      "GrLivArea               0.340011\n",
      "OverallCond_3          -0.156932\n",
      "KitchenAbvGr           -0.153132\n",
      "Functional_Maj2        -0.148665\n",
      "Neighborhood_StoneBr    0.110758\n",
      "Neighborhood_Crawfor    0.108176\n",
      "RoofMatl_WdShngl        0.093379\n",
      "SaleType_New            0.091178\n",
      "Heating_Grav           -0.086204\n",
      "Neighborhood_NridgHt    0.085754\n",
      "dtype: float64\n",
      "Pipeline(steps=[('lasso', Lasso(alpha=0.0001, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=True, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False))])\n",
      "\n",
      "Method is Lasso Regression\n",
      "Root Mean Square Error 0.120335683338\n",
      "\n",
      "#################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########### Lasso Regression #############\n",
    "\n",
    "# Define model pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Lasso\n",
    "pipeline_lasso = Pipeline(steps=[('lasso',Lasso(alpha=0.0001, normalize=True))])\n",
    "parameters_lasso = {'lasso__alpha': (0.00005, 0.0001)}\n",
    "\n",
    "# Print largest coefficients in model\n",
    "pipeline_lasso.fit(train_x, train_y)\n",
    "coef = pd.Series(pipeline_lasso.named_steps['lasso'].coef_, index=df_train_x.columns).sort_values()\n",
    "print\"The top ten most significant coefficients for lasso regression are:\"\n",
    "imp_coef = abs(coef).nlargest(10)\n",
    "print coef.loc[imp_coef.index]\n",
    "\n",
    "# Search for best parameters\n",
    "optimize_and_evaluate_model(pipeline_lasso, parameters_lasso, \"Lasso Regression\", train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Feature Importance Analysis\n",
      "The top ten most significant coefficients for random forest regression are:\n",
      "OverallQual     0.079252\n",
      "GrLivArea       0.152157\n",
      "GrLivArea_sq    0.218719\n",
      "GarageYrBlt     0.262324\n",
      "sqYearBuilt     0.304416\n",
      "TotalBsmtSF     0.344887\n",
      "YearBuilt       0.385255\n",
      "sqGarageCars    0.424084\n",
      "GarageCars      0.461972\n",
      "1stFlrSF        0.496030\n",
      "dtype: float64\n",
      "Pipeline(steps=[('randomforest', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=40, max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=80, n_jobs=1,\n",
      "           oob_score=False, random_state=0, verbose=0, warm_start=False))])\n",
      "\n",
      "Method is Random Forest Regression\n",
      "Root Mean Square Error 0.142368909872\n",
      "\n",
      "#################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Random Forest Feature Importance Analysis\"\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "pipeline_forest = Pipeline(steps=[('randomforest',RandomForestRegressor(n_estimators=80, max_features=20, random_state=0))])\n",
    "parameters_forest = {'randomforest__n_estimators': (60, 80),\n",
    "                     'randomforest__max_features': (20, 40)}\n",
    "\n",
    "# Print features by importance in model\n",
    "pipeline_forest.fit(train_x,train_y)\n",
    "importances = pd.Series(pipeline_forest.named_steps['randomforest'].feature_importances_, index=df_train_x.columns).sort_values()\n",
    "print\"The top ten most significant coefficients for random forest regression are:\"\n",
    "top_importances = importances.nlargest(10)\n",
    "print top_importances.cumsum()\n",
    "\n",
    "# Search for best parameters\n",
    "optimize_and_evaluate_model(pipeline_forest, parameters_forest, \"Random Forest Regression\", train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada Boosting Regression\n",
      "The top ten most significant coefficients for random ada boost regression are:\n",
      "OverallQual      0.320269\n",
      "GrLivArea        0.492357\n",
      "GrLivArea_sq     0.620169\n",
      "1stFlrSF         0.663917\n",
      "TotalBsmtSF      0.702550\n",
      "YearRemodAdd     0.733894\n",
      "CentralAir_Y     0.758380\n",
      "sqGarageYrBlt    0.778045\n",
      "GarageCars       0.793958\n",
      "BsmtFinSF1       0.808965\n",
      "dtype: float64\n",
      "Pipeline(steps=[('ada', AdaBoostRegressor(base_estimator=None, learning_rate=1, loss='linear',\n",
      "         n_estimators=40, random_state=None))])\n",
      "\n",
      "Method is AdaBoost Regression\n",
      "Root Mean Square Error 0.174674706755\n",
      "\n",
      "#################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########### AdaBoosting Regression #############\n",
    "\n",
    "print \"Ada Boosting Regression\"\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "pipeline_ada = Pipeline(steps=[('ada',AdaBoostRegressor(n_estimators=50,learning_rate=1.0))])\n",
    "parameters_ada = {'ada__n_estimators': (40, 50),\n",
    "                  'ada__learning_rate': (0.1, 1)}\n",
    "\n",
    "# Print features by importance in model\n",
    "pipeline_ada.fit(train_x,train_y)\n",
    "importances = pd.Series(pipeline_ada.named_steps['ada'].feature_importances_, index=df_train_x.columns).sort_values()\n",
    "print\"The top ten most significant coefficients for random ada boost regression are:\"\n",
    "top_importances = importances.nlargest(10)\n",
    "print top_importances.cumsum()\n",
    "\n",
    "# Search for best parameters\n",
    "optimize_and_evaluate_model(pipeline_ada, parameters_ada, \"AdaBoost Regression\", train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keyangzhang/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [201] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=80, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('kbest', SelectKBest(k=5, score_func=<function f_classif at 0x1182fe578>))],\n",
      "       transformer_weights=None)), ('lr', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))])\n",
      "\n",
      "Method is PCA plus Linear Regression\n",
      "Root Mean Square Error 0.128993631282\n",
      "\n",
      "#################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########### PCA Regression #############\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "selection = SelectKBest(k=5)\n",
    "combined_features = FeatureUnion([(\"pca\", pca), (\"kbest\", selection)])\n",
    "\n",
    "pipeline_pca = Pipeline(steps=[(\"features\", combined_features), (\"lr\", LinearRegression())])\n",
    "parameters_pca = {'features__pca__n_components': (20,40,80)}\n",
    "\n",
    "# Search for best parameters\n",
    "optimize_and_evaluate_model(pipeline_pca, parameters_pca, \"PCA plus Linear Regression\", train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 245)\n"
     ]
    }
   ],
   "source": [
    "print df_train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
